<font face = "Times New Roman">

## Word Embedding

### One-hot Representation

**Problem**

* Dot product is zero. 

  Our query and document vectors are orthogonal

  There is no **natural notion** of similarity in a set of one-hot vectors

* High dimension, much space.

**Solve**

Reduce the dimensions (CNN)

### Speech Context

* Method 1: continuous bag-of-word $(CBOW)$â€‹

* Method 2: skip-gram $(SG)$

![1](1.png)![2](2.png)

## Recurent Neural Network
* Mostly refer to slides.
</font>