{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d0a452",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2599b",
   "metadata": {},
   "source": [
    "### 1. 安装gensim库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b39cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting gensim\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/9f/10/0a737b7f935a14ac49d12187530952805978e1be506212b7c66d15962e27/gensim-4.2.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.5.4)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/65/12/cc24847b4b0b124501a33cd8f7963f79f6f6584bc7f2f4fc16bbbaa54c8f/smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.21.2)\n",
      "Requirement already satisfied: wrapt in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-7.0.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479be3b3",
   "metadata": {},
   "source": [
    "### 2. 同步数据至本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e95ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/word_embedding/corpus.txt\", dst_url='corpus.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8458d12",
   "metadata": {},
   "source": [
    "### 3. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490c81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865f3b8",
   "metadata": {},
   "source": [
    "### 4. 定义输入、输出文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02206a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"corpus.txt\"\n",
    "out_embedding_file = \"embedding.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec554e65",
   "metadata": {},
   "source": [
    "### 5. 查看函数文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961dc6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
       "\n",
       "Warnings\n",
       "--------\n",
       "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
       "such as lambda functions etc.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
       "to reduce memory.\n",
       "\n",
       "The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "vector_size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "epochs : int, optional\n",
       "    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "shrink_windows : bool, optional\n",
       "    New in 4.1. Experimental.\n",
       "    If True, the effective window size is uniformly sampled from  [1, `window`]\n",
       "    for each target word during training, to match the original word2vec algorithm's\n",
       "    approximate weighting of context words by distance. Otherwise, the effective\n",
       "    window size is always fixed to `window` words to either side.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Doc2Vec, FastText\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde7777",
   "metadata": {},
   "source": [
    "### 6. 词向量训练并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e5d3506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 434258 word types from a corpus of 8163235 raw words and 6566 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 99249 unique words (22.85% of original 434258, drops 335009)', 'datetime': '2024-03-27T11:50:06.736740', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7661631 word corpus (93.86% of original 8163235, drops 501604)', 'datetime': '2024-03-27T11:50:06.738705', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 434258 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7062927.526606419 word corpus (92.2%% of prior 7661631)', 'datetime': '2024-03-27T11:50:07.969510', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 99249 words and 100 dimensions: 129023700 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-27T11:50:10.134804', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 192 workers on 99249 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-03-27T11:50:10.136865', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 0.35% examples, 6997 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 1.20% examples, 16684 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 1.61% examples, 17808 words/s, in_qsize -1, out_qsize 9\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 19.52% examples, 205646 words/s, in_qsize -1, out_qsize 5\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 23.90% examples, 211472 words/s, in_qsize -1, out_qsize 5\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 24.46% examples, 189098 words/s, in_qsize -1, out_qsize 191\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 111.88% examples, 595605 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 9202724 raw words (7444343 effective words) took 12.5s, 595513 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 0.35% examples, 7729 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 2.51% examples, 43672 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 4.57% examples, 61137 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 4.96% examples, 52260 words/s, in_qsize -1, out_qsize 137\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 69.74% examples, 557025 words/s, in_qsize -1, out_qsize 5\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 9202724 raw words (7445499 effective words) took 9.9s, 751761 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 0.37% examples, 4097 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 1.54% examples, 13815 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 6.21% examples, 47651 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 11.59% examples, 81535 words/s, in_qsize -1, out_qsize 17\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 12.15% examples, 76776 words/s, in_qsize -1, out_qsize 249\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 9202724 raw words (7444890 effective words) took 12.7s, 585754 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 0.35% examples, 3819 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 1.42% examples, 13424 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 3.84% examples, 29864 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 7.95% examples, 57403 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 18.00% examples, 113146 words/s, in_qsize -1, out_qsize 117\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 9202724 raw words (7445162 effective words) took 13.2s, 562968 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 0.30% examples, 3912 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 1.78% examples, 17377 words/s, in_qsize -1, out_qsize 3\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 4.25% examples, 34167 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 12.41% examples, 81303 words/s, in_qsize -1, out_qsize 245\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 111.88% examples, 572544 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 9202724 raw words (7444918 effective words) took 13.0s, 572465 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 46013620 raw words (37224812 effective words) took 86.2s, 431984 effective words/s', 'datetime': '2024-03-27T11:51:36.309683', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=99249, vector_size=100, alpha=0.8>', 'datetime': '2024-03-27T11:51:36.310993', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'created'}\n",
      "INFO:gensim.models.keyedvectors:storing 99249x100 projection weights into embedding.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.wv.save_word2vec_format(out_embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b15f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt  embedding.txt  word2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e642a4e",
   "metadata": {},
   "source": [
    "### 7. 加载离线词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd38096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from embedding.txt\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (99249, 100) matrix of type float32 from embedding.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-03-27T11:52:03.237400', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 22:05:51) \\n[GCC 9.4.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fc777",
   "metadata": {},
   "source": [
    "获取单个词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bcb52cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.96137834, -0.23067355,  3.1922088 , -3.5002666 , -1.5135725 ,\n",
       "        2.8573692 , -1.1729655 , -0.45207393,  0.77694273,  5.5679665 ,\n",
       "        2.2340856 ,  4.8480687 , -0.54213023,  0.0193974 , -0.9376919 ,\n",
       "       -0.39847037,  0.93367904,  1.5010141 ,  2.289926  ,  3.3319795 ,\n",
       "        1.5700797 ,  1.9068444 , -0.7992846 ,  0.39034274,  0.26855385,\n",
       "       -2.604558  ,  1.2158004 ,  1.7940377 , -1.1394768 , -2.5749233 ,\n",
       "       -4.7151136 , -3.338187  , -1.8117958 , -1.1645187 ,  0.8603604 ,\n",
       "       -2.8884757 , -3.443384  ,  4.008008  , -0.16843796,  2.4852476 ,\n",
       "        3.5308914 ,  0.01191678,  0.46323293,  2.6151037 , -1.3842027 ,\n",
       "       -2.3022392 ,  3.9343739 , -3.7834537 ,  2.2835433 ,  1.9086242 ,\n",
       "        1.1604811 ,  1.0526192 , -1.1386466 , -1.8117647 , -3.5456314 ,\n",
       "        1.0661595 ,  2.670567  , -1.6311446 ,  0.38272265,  1.8802515 ,\n",
       "        1.2993425 , -0.930947  , -0.5783536 ,  0.78189725, -0.32224166,\n",
       "        1.6660247 ,  0.05364052,  0.22718306,  0.45890036, -3.9079938 ,\n",
       "       -2.6283958 , -2.8491669 , -0.15903011,  0.82385284,  0.55017483,\n",
       "        0.28034666,  0.2812887 ,  2.9279435 , -0.4887835 , -0.00870828,\n",
       "        1.9349993 , -0.13197449, -1.0987545 , -2.5169141 ,  2.7653427 ,\n",
       "        0.9458307 , -0.20362824, -1.5414759 , -2.2118907 , -3.6660771 ,\n",
       "        0.531146  , -2.0809736 , -0.7854844 , -1.0487887 , -4.165273  ,\n",
       "       -2.5818238 , -0.568762  ,  0.89809185,  0.30586684,  0.8473722 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['中国']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ac5e7",
   "metadata": {},
   "source": [
    "### 8. 相似度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e35555c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融\n",
      "[('科场', 0.9738190174102783), ('内侍', 0.9721248149871826), ('辞职信', 0.9690426588058472), ('籾', 0.9664568901062012), ('伊达尔戈', 0.9659194350242615), ('货站', 0.9656187295913696), ('皇贵', 0.965103268623352), ('元至元', 0.9649425148963928), ('观音大士', 0.9648800492286682), ('法库县', 0.9645453691482544)]\n",
      "喜欢\n",
      "[('俚谚', 0.9639517664909363), ('之鹰', 0.9627872109413147), ('俞国华', 0.9618333578109741), ('硕塞', 0.9614347815513611), ('冰原', 0.9597957730293274), ('插进', 0.9595819711685181), ('阿瑟', 0.9595571160316467), ('经张', 0.9590895771980286), ('首飞', 0.9581519961357117), ('凤阳', 0.9577733278274536)]\n",
      "中国\n",
      "[('提倡者', 0.9652292132377625), ('杨根思', 0.9648443460464478), ('赋中', 0.9648005366325378), ('羌语', 0.9641041159629822), ('第二章', 0.9637407064437866), ('螺栓', 0.9616395831108093), ('先天性免疫', 0.9613238573074341), ('初封', 0.9609901905059814), ('罗常培', 0.9596619606018066), ('诉求', 0.9591411352157593)]\n",
      "北京\n",
      "[('冒险游戏', 0.961117148399353), ('巴拿马运河', 0.9604386687278748), ('科学计算', 0.9598163962364197), ('曼彻斯特大学', 0.9590226411819458), ('力荐', 0.9568527936935425), ('南汉宸', 0.9566811323165894), ('信实', 0.9548183679580688), ('混乱状态', 0.9537748098373413), ('小面', 0.9533365368843079), ('暖湿气流', 0.9530967473983765)]\n",
      "南开\n",
      "[('残雪', 0.976757824420929), ('科幻电影', 0.9714664816856384), ('朱然', 0.969613254070282), ('火车票', 0.9664067625999451), ('遭人', 0.965401828289032), ('伦茨', 0.9635688662528992), ('光反应', 0.9634864926338196), ('澄观', 0.9624661803245544), ('懽', 0.9623029828071594), ('缴付', 0.9607928991317749)]\n",
      "浙江大学\n",
      "[('人性化', 0.9037673473358154), ('地幔', 0.8795541524887085), ('穆克', 0.8784704208374023), ('五环', 0.8782944679260254), ('杰布', 0.8689160943031311), ('剥离', 0.8657026290893555), ('斯温', 0.8611894249916077), ('灯笼', 0.8611074686050415), ('反党集团', 0.8596020340919495), ('圣公宗', 0.8547413945198059)]\n",
      "神经\n",
      "[('黎利', 0.9710570573806763), ('反舰导弹', 0.9661895632743835), ('白背', 0.9635065793991089), ('国对', 0.9608851671218872), ('汉帝', 0.9607863426208496), ('几层', 0.9582294225692749), ('贝尔', 0.9578816294670105), ('词藻', 0.9578383564949036), ('蔬菜水果', 0.9575150609016418), ('吴时', 0.9568917751312256)]\n",
      "网络\n",
      "[('邱奇', 0.941702127456665), ('渔山', 0.9368969202041626), ('检讨', 0.9358852505683899), ('工业区', 0.9346534013748169), ('兽力', 0.9330962896347046), ('纯朴', 0.9319484233856201), ('导演奖', 0.9315592646598816), ('核酶', 0.9286218881607056), ('中轴线', 0.9283182621002197), ('太岁', 0.9277064800262451)]\n",
      "金庸\n",
      "[('医药学家', 0.9705321192741394), ('一千元', 0.9534820914268494), ('交趾', 0.9521581530570984), ('咖啡馆', 0.9499489068984985), ('双子叶', 0.9492812156677246), ('建湖', 0.9479783773422241), ('干旱气候', 0.9472019076347351), ('亚洲杯', 0.94623863697052), ('下落', 0.9458320140838623), ('营养器官', 0.9442867636680603)]\n"
     ]
    }
   ],
   "source": [
    "# 这里是特意选取了不合适的initial learning rate 来演示 过大的learning rate会导致模型不稳定\n",
    "testwords = ['金融', '喜欢', \"中国\", \"北京\",\"南开\",\"浙江大学\",\"神经\",\"网络\",\"金庸\"]\n",
    "for word in testwords:\n",
    "    res = word2vec_model.most_similar(word)\n",
    "    print (word)\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db71dd",
   "metadata": {},
   "source": [
    "### 9. 词向量文件回传obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"embedding.txt\", dst_url='s3://ascend-zyjs-dcyang/nlp/word_embedding/embedding.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
