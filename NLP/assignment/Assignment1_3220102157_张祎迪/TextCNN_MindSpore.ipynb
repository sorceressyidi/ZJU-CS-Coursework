{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.17.3-d858ff4a\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "# 请替换成自己的obs路径\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\", dst_url='./data/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'Ascend',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[ 1292, 16517,    15 ...     0,     0,     0],\n",
      " [   15,  7358,  4355 ...     0,     0,     0],\n",
      " [ 1799,  2182,  2183 ...     0,     0,     0],\n",
      " ...\n",
      " [ 5021,   243,  3431 ...     0,     0,     0],\n",
      " [    0,   889,  1355 ...     0,     0,     0],\n",
      " [   75,  8285,  2769 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, \n",
      " 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, \n",
      " 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])}\n",
      "[  15 7358 4355   77   11  582    4 7359  313 1442 7360   10 5493   91\n",
      "   15  273  155  351  660  906    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-0.00896155  0.00767637 -0.00254218 ...  0.00805752  0.00462216\n",
      "        -0.01470062]\n",
      "       [ 0.00373914 -0.00506907 -0.00610072 ... -0.01878295  0.01065604\n",
      "         0.00233409]\n",
      "       [-0.01450504 -0.00451986  0.00040308 ...  0.00161393  0.00110012\n",
      "         0.01152107]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.02367887 -0.00311962  0.01162793 ...  0.00094095 -0.00296565\n",
      "        -0.00182325]\n",
      "       [ 0.00011851  0.00638012 -0.00996731 ...  0.00448265  0.016368\n",
      "        -0.00320697]\n",
      "       [-0.00503235 -0.00495745  0.00280931 ... -0.0049577   0.00376548\n",
      "         0.01127809]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00288061  0.00861202  0.01056818 ... -0.0062167   0.00063382\n",
      "         0.00766866]\n",
      "       [-0.00328799 -0.00092185 -0.00284319 ... -0.01924388 -0.02517372\n",
      "         0.01049215]\n",
      "       [ 0.00908852  0.00908642  0.0060694  ...  0.00562322 -0.00729331\n",
      "        -0.00996371]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.00155885  0.00238307 -0.00707258 ...  0.00639367 -0.00478611\n",
      "        -0.00332186]\n",
      "       [-0.0067476  -0.00717538 -0.00605535 ... -0.0065589   0.01025294\n",
      "        -0.00069957]\n",
      "       [-0.01389091  0.00874626  0.00244971 ...  0.00222629 -0.00734142\n",
      "         0.01566721]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.02000639 -0.00265024 -0.01138989 ...  0.01332899  0.00659901\n",
      "         0.01278557]\n",
      "       [ 0.00815881  0.00847621 -0.00928253 ...  0.00863612 -0.01442512\n",
      "        -0.00680711]\n",
      "       [ 0.00719303  0.00950829 -0.00118836 ... -0.00215995 -0.00027187\n",
      "        -0.01110245]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00634184 -0.00065266 -0.00097598 ...  0.00158555 -0.0140077\n",
      "        -0.01282323]\n",
      "       [-0.01090841 -0.00098034 -0.00279723 ...  0.0020403   0.01622084\n",
      "        -0.00303959]\n",
      "       [ 0.00437354  0.00172317 -0.00363193 ...  0.00431407  0.00508054\n",
      "        -0.01235182]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 0.00041074 -0.00612098 -0.01315296 ... -0.01607839  0.00059556\n",
      "         0.00682876]\n",
      "       [ 0.00459455  0.01610972  0.01973294 ... -0.00781943 -0.00034262\n",
      "        -0.00139319]\n",
      "       [-0.01453746  0.00262366  0.00547988 ...  0.00292326  0.00052146\n",
      "         0.00694971]\n",
      "       [-0.00986763 -0.00342298 -0.00385826 ... -0.005071    0.00192198\n",
      "        -0.00309106]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00366133  0.00691936  0.00821531 ...  0.00106495  0.00427346\n",
      "         0.01176313]\n",
      "       [ 0.00631864  0.01123485 -0.00303427 ... -0.00557339 -0.00040636\n",
      "         0.00233417]\n",
      "       [ 0.01165047  0.00322817 -0.01778503 ... -0.00483399 -0.00144649\n",
      "        -0.00337031]\n",
      "       [ 0.01498169  0.01667083 -0.01003805 ... -0.0126855  -0.00057486\n",
      "        -0.00201677]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00610491 -0.01750218 -0.00123562 ... -0.00505204 -0.00343655\n",
      "        -0.00365435]\n",
      "       [-0.01634141  0.00372833  0.00148709 ...  0.01354341  0.00274937\n",
      "         0.00195294]\n",
      "       [-0.00273439  0.01733441 -0.00787553 ...  0.01748989  0.01009944\n",
      "         0.0064335 ]\n",
      "       [ 0.0188036  -0.00405708 -0.00641867 ...  0.00690732  0.0094523\n",
      "         0.01847732]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.0087723   0.0049812  -0.01325759 ...  0.00870206  0.02513452\n",
      "         0.00285597]\n",
      "       [-0.01721388  0.00374042 -0.00417353 ...  0.00566066  0.01755654\n",
      "        -0.00126282]\n",
      "       [ 0.00613152  0.0032891  -0.00128879 ... -0.00590204  0.01012614\n",
      "         0.01036489]\n",
      "       [-0.01986193  0.00138674  0.00632495 ... -0.01146492  0.00524518\n",
      "         0.00192592]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.01575093 -0.0136401  -0.0127474  ... -0.00462734  0.00768396\n",
      "         0.01291676]\n",
      "       [-0.00275464 -0.0015708   0.01229004 ...  0.00283488 -0.01969917\n",
      "        -0.00150424]\n",
      "       [ 0.0061002  -0.00801683  0.00765541 ...  0.01450451 -0.00169575\n",
      "         0.01613855]\n",
      "       [-0.00795217  0.00489015 -0.00198061 ... -0.00194394  0.00154273\n",
      "        -0.00373504]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.01384334  0.01433394 -0.009425   ... -0.0148033  -0.0048253\n",
      "         0.00956862]\n",
      "       [ 0.00766949 -0.00820082 -0.00838677 ...  0.007624    0.00421699\n",
      "         0.0044939 ]\n",
      "       [ 0.01183361  0.00913515  0.0104312  ...  0.0044564  -0.01411647\n",
      "        -0.01607594]\n",
      "       [-0.0142177  -0.01155186  0.00291818 ... -0.00176045  0.00820715\n",
      "        -0.00726206]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 7.12171197e-03 -2.65951618e-03 -9.57109034e-03 ...  3.86716973e-04\n",
      "         2.87132338e-03  1.19399084e-02]\n",
      "       [ 1.66255154e-03  7.40727046e-05 -8.08324106e-03 ...  6.22381596e-03\n",
      "         3.84640717e-03 -2.16062460e-03]\n",
      "       [ 2.53685121e-03  5.77264279e-03 -7.15635950e-03 ... -1.06619077e-03\n",
      "        -1.34950234e-02 -3.80991865e-03]\n",
      "       [ 7.94818345e-03 -6.01296546e-03 -2.69159151e-04 ... -8.82253610e-03\n",
      "        -1.10526651e-03  1.01862345e-02]\n",
      "       [-1.84087586e-02  1.90758221e-02  2.66307057e-03 ...  1.91386212e-02\n",
      "        -3.41447699e-03 -3.90691310e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-8.42669420e-03 -6.22557802e-03  4.29162476e-03 ... -8.44861474e-03\n",
      "         1.90231968e-02  2.87506673e-02]\n",
      "       [-4.51621739e-03  5.21083223e-03  9.23976861e-03 ...  1.61350835e-02\n",
      "         1.54919671e-02 -7.63974385e-04]\n",
      "       [-1.43091939e-02  1.28996931e-02 -1.03379868e-03 ...  5.24468580e-03\n",
      "         6.09424547e-04 -3.70832649e-03]\n",
      "       [ 8.74659140e-03  1.95984519e-03  1.92126026e-03 ... -1.09609729e-02\n",
      "         5.12028905e-03 -1.40142459e-02]\n",
      "       [ 1.80374540e-03 -1.27007337e-02  1.70680694e-04 ... -9.70415305e-03\n",
      "        -6.85226778e-03 -1.20518571e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.69735921e-02 -4.44760500e-03 -6.77071931e-03 ... -2.39455886e-02\n",
      "        -4.23943345e-03 -3.91197531e-03]\n",
      "       [ 1.15182502e-02  1.86562184e-02 -1.11029968e-02 ...  1.65754426e-02\n",
      "         2.53617880e-03 -3.18058394e-03]\n",
      "       [ 2.57154722e-02 -2.60627363e-03  2.70550395e-03 ...  2.16271874e-04\n",
      "        -8.34948476e-03  1.09241437e-02]\n",
      "       [ 6.81937300e-03 -5.00079617e-03  1.54812168e-02 ...  2.37655230e-02\n",
      "         1.47388375e-03  1.85400769e-02]\n",
      "       [ 1.36771761e-02 -2.04319209e-02  5.67609305e-03 ...  1.67437419e-02\n",
      "        -1.00544374e-02  9.95612121e-04]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 2.23472845e-02  9.76427272e-03 -1.61697797e-03 ... -3.21644545e-03\n",
      "        -1.09179998e-02 -1.74703740e-03]\n",
      "       [ 1.33315604e-02  3.47293587e-03  8.32721498e-03 ...  4.77092899e-03\n",
      "         1.68153364e-02  7.35868089e-05]\n",
      "       [ 8.01685965e-04  1.99486725e-02 -3.86825384e-04 ...  3.36896745e-03\n",
      "         1.12502836e-02 -1.23073421e-02]\n",
      "       [-5.21224830e-03  1.23599055e-03 -1.14258155e-02 ...  7.16219097e-03\n",
      "        -7.80996773e-03  1.01646688e-03]\n",
      "       [-1.06822944e-03 -2.22441158e-03  6.59494381e-03 ...  5.08317119e-03\n",
      "        -1.15515171e-02 -6.07245369e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.61223519e-03  1.02710060e-03 -1.41113168e-02 ...  2.56015621e-02\n",
      "         1.41991349e-02  3.92378774e-03]\n",
      "       [ 3.35773872e-03  8.17623176e-03  6.10565720e-03 ...  1.25361811e-02\n",
      "        -8.20273068e-03 -2.32855696e-03]\n",
      "       [ 2.50458391e-03 -3.72864003e-03 -1.02304127e-02 ... -4.48465068e-03\n",
      "         4.52279905e-03  2.35997303e-03]\n",
      "       [-2.64380756e-03  8.83173256e-04  3.69469024e-04 ...  9.54047404e-03\n",
      "        -5.14389668e-03 -1.98809952e-02]\n",
      "       [ 1.63462739e-02  5.41072479e-03 -1.09237218e-02 ... -9.09786485e-03\n",
      "         6.71160780e-03  1.75966695e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.62939297e-03  4.96242940e-03  4.20289300e-03 ... -1.40038310e-02\n",
      "         1.11110518e-02  7.06045609e-03]\n",
      "       [ 8.08819663e-03  9.89576429e-03  4.94193193e-03 ...  4.96052951e-03\n",
      "         1.71181764e-05 -1.04516279e-02]\n",
      "       [ 3.74902831e-03  2.20590504e-03  2.00286973e-03 ...  3.98809207e-04\n",
      "        -7.26069324e-03  8.45766813e-03]\n",
      "       [-1.23681761e-02  9.15099867e-03  1.37747149e-03 ... -1.28311627e-02\n",
      "         1.46495299e-02 -1.59332752e-02]\n",
      "       [ 1.14444233e-02  4.19510109e-03  1.00239906e-02 ... -3.73698131e-04\n",
      "        -2.60863989e-03  1.74634326e-02]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.07538004219532013\n",
      "epoch time: 31894.220 ms, per step time: 53.514 ms\n",
      "epoch: 2 step: 596, loss is 0.003268774366006255\n",
      "epoch time: 4835.746 ms, per step time: 8.114 ms\n",
      "epoch: 3 step: 596, loss is 0.0005181985907256603\n",
      "epoch time: 4776.394 ms, per step time: 8.014 ms\n",
      "epoch: 4 step: 596, loss is 0.0005404085968621075\n",
      "epoch time: 4841.885 ms, per step time: 8.124 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n",
      "accuracy:  {'acc': 0.7548828125}\n"
     ]
    }
   ],
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)\n",
    "net.set_train(False)\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n",
      "Negative comments\n",
      "Negative comments\n",
      "Positive comments\n",
      "Positive comments\n",
      "Positive comments\n",
      "Positive comments\n",
      "Negative comments\n",
      "Negative comments\n",
      "Negative comments\n",
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review = [\"the movie is so boring\",\n",
    "          \"tedious\",\n",
    "          \"not that bad\",\n",
    "          \"wonderful\",\n",
    "          \"quite meaningful\",\n",
    "          \"rather attractive\",\n",
    "          \"not that good but overall acceptable\",\n",
    "          \"overall acceptable but not that good\",\n",
    "          \"just so so still need improvement\",\n",
    "          \"the actor makes me feel out of place\",\n",
    "          \"without a single flaw\"\n",
    "         ]\n",
    "for review_en in review :\n",
    "    inference(review_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
