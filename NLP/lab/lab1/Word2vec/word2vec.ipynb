{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Word Embedding"]},{"cell_type":"markdown","metadata":{},"source":["### 1. 安装gensim库"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gensim in /Users/lily/anaconda3/lib/python3.11/site-packages (4.3.0)\n","Requirement already satisfied: numpy>=1.18.5 in /Users/lily/anaconda3/lib/python3.11/site-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /Users/lily/anaconda3/lib/python3.11/site-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /Users/lily/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n","Collecting FuzzyTM>=0.4.0 (from gensim)\n","  Downloading FuzzyTM-2.0.5-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: pandas in /Users/lily/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (2.1.4)\n","Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n","  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lily/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /Users/lily/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /Users/lily/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n","Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n","  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n","Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n","  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing-extensions in /Users/lily/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.9.0)\n","Requirement already satisfied: six>=1.5 in /Users/lily/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n","Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n","  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hDownloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n","Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: fst-pso, miniful\n","  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20431 sha256=82a667fabeb78893149253c2ab6ec14fa860b8b2cc2404fe8653c4eb6caaa352\n","  Stored in directory: /Users/lily/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n","  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=4f093c64a14ccb4fbdec36c21b12d67fbee2ece9b2fb2e54fb7ffe0dd1816539\n","  Stored in directory: /Users/lily/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n","Successfully built fst-pso miniful\n","Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n","Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.3.1 simpful-2.12.0\n"]}],"source":["! pip install gensim"]},{"cell_type":"markdown","metadata":{},"source":["### 2. 同步数据至本地"]},{"cell_type":"markdown","metadata":{},"source":["### 3. 导入依赖库"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["from multiprocessing import cpu_count\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors"]},{"cell_type":"markdown","metadata":{},"source":["### 4. 定义输入、输出文件路径"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["corpus_file = \"corpus.txt\"\n","out_embedding_file = \"embedding.txt\""]},{"cell_type":"markdown","metadata":{},"source":["### 5. 查看函数文档"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n","\n","Warnings\n","--------\n","This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n","such as lambda functions etc.\n","\u001b[0;31mInit docstring:\u001b[0m\n","Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n","\n","Once you're finished training a model (=no more updates, only querying)\n","store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n","to reduce memory.\n","\n","The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n","\n","The trained word vectors can also be stored/loaded from a format compatible with the\n","original word2vec implementation via `self.wv.save_word2vec_format`\n","and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n","\n","Parameters\n","----------\n","sentences : iterable of iterables, optional\n","    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n","    consider an iterable that streams the sentences directly from disk/network.\n","    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n","    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n","    See also the `tutorial on data streaming in Python\n","    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n","    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n","    in some other way.\n","corpus_file : str, optional\n","    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n","    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n","    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n","vector_size : int, optional\n","    Dimensionality of the word vectors.\n","window : int, optional\n","    Maximum distance between the current and predicted word within a sentence.\n","min_count : int, optional\n","    Ignores all words with total frequency lower than this.\n","workers : int, optional\n","    Use these many worker threads to train the model (=faster training with multicore machines).\n","sg : {0, 1}, optional\n","    Training algorithm: 1 for skip-gram; otherwise CBOW.\n","hs : {0, 1}, optional\n","    If 1, hierarchical softmax will be used for model training.\n","    If 0, and `negative` is non-zero, negative sampling will be used.\n","negative : int, optional\n","    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n","    should be drawn (usually between 5-20).\n","    If set to 0, no negative sampling is used.\n","ns_exponent : float, optional\n","    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n","    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n","    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n","    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n","    other values may perform better for recommendation applications.\n","cbow_mean : {0, 1}, optional\n","    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n","alpha : float, optional\n","    The initial learning rate.\n","min_alpha : float, optional\n","    Learning rate will linearly drop to `min_alpha` as training progresses.\n","seed : int, optional\n","    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n","    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n","    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n","    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n","    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n","max_vocab_size : int, optional\n","    Limits the RAM during vocabulary building; if there are more unique\n","    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n","    Set to `None` for no limit.\n","max_final_vocab : int, optional\n","    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n","    min_count is more than the calculated min_count, the specified min_count will be used.\n","    Set to `None` if not required.\n","sample : float, optional\n","    The threshold for configuring which higher-frequency words are randomly downsampled,\n","    useful range is (0, 1e-5).\n","hashfxn : function, optional\n","    Hash function to use to randomly initialize weights, for increased training reproducibility.\n","epochs : int, optional\n","    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n","trim_rule : function, optional\n","    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n","    be trimmed away, or handled using the default (discard if word count < min_count).\n","    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n","    or a callable that accepts parameters (word, count, min_count) and returns either\n","    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n","    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n","    model.\n","\n","    The input parameters are of the following types:\n","        * `word` (str) - the word we are examining\n","        * `count` (int) - the word's frequency count in the corpus\n","        * `min_count` (int) - the minimum count threshold.\n","sorted_vocab : {0, 1}, optional\n","    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n","    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n","batch_words : int, optional\n","    Target size (in words) for batches of examples passed to worker threads (and\n","    thus cython routines).(Larger batches will be passed if individual\n","    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n","compute_loss: bool, optional\n","    If True, computes and stores loss value which can be retrieved using\n","    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n","callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n","    Sequence of callbacks to be executed at specific stages during training.\n","shrink_windows : bool, optional\n","    New in 4.1. Experimental.\n","    If True, the effective window size is uniformly sampled from  [1, `window`]\n","    for each target word during training, to match the original word2vec algorithm's\n","    approximate weighting of context words by distance. Otherwise, the effective\n","    window size is always fixed to `window` words to either side.\n","\n","Examples\n","--------\n","Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n","\n",".. sourcecode:: pycon\n","\n","    >>> from gensim.models import Word2Vec\n","    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n","    >>> model = Word2Vec(sentences, min_count=1)\n","\n","Attributes\n","----------\n","wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n","    This object essentially contains the mapping between words and embeddings. After training, it can be used\n","    directly to query those embeddings in various ways. See the module level docstring for examples.\n","\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     Doc2Vec, FastText"]}],"source":["?Word2Vec"]},{"cell_type":"markdown","metadata":{},"source":["### 6. 词向量训练并保存"]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true},"outputs":[],"source":["model = Word2Vec(corpus_file=corpus_file, vector_size=100, window=5, min_count=5, workers=cpu_count(), sg=1)\n","model.wv.save_word2vec_format(out_embedding_file, binary=False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["corpus.txt  embedding.txt\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{},"source":["### 7. 加载离线词向量"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word2vec_model = KeyedVectors.load_word2vec_format(\"embedding.txt\")"]},{"cell_type":"markdown","metadata":{},"source":["获取单个词的词向量"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["array([-0.11945462,  0.80811197, -0.2192669 , -0.35121506, -0.2546237 ,\n","       -0.00980086,  0.7964671 ,  0.36271024,  0.31742495, -0.3251099 ,\n","       -0.13367757, -0.512894  ,  0.18774055,  0.28745985, -0.08601924,\n","        0.0150877 ,  0.73333174, -0.6884307 ,  0.09896889, -0.4288761 ,\n","        0.7132578 ,  0.396008  ,  0.4390735 , -0.09943724,  0.20115437,\n","        0.07450946,  0.01946275,  0.5919555 ,  0.11550876, -0.2978358 ,\n","       -0.18850829, -0.279784  , -0.56403273, -0.6189067 , -0.2669414 ,\n","       -0.1924906 ,  0.56643754,  0.05672867,  0.20835687,  0.53800774,\n","        0.05601315,  0.01130022, -0.13399325,  0.01399448,  0.44789463,\n","       -0.16624065,  0.23680332,  0.48087487,  0.2066727 ,  0.30111927,\n","        0.23712948, -0.01020607,  0.09069001, -0.35201445,  0.30945036,\n","       -0.29315245,  0.0818078 , -0.15353864, -0.21203907,  0.33678278,\n","       -0.26471385, -0.40392622,  0.12019241,  0.37057823,  0.10111269,\n","        0.29821014,  0.1682322 ,  0.3837336 , -0.48044497,  0.00659311,\n","       -0.12619178,  0.08527908, -0.10650562,  0.01504959, -0.17726953,\n","       -0.42102095,  0.4874898 ,  0.4325759 ,  0.54365766, -0.02363636,\n","       -0.3604603 ,  0.21117018, -0.56193644, -0.2412196 ,  0.33035398,\n","       -0.09440871,  0.8251365 ,  0.16836917, -0.2605197 ,  0.40633464,\n","        0.7236247 , -0.36978406,  0.37283292,  0.40109146, -0.09860536,\n","       -0.48287332,  0.05875314,  0.15065585, -0.8095557 ,  0.62114567],\n","      dtype=float32)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["word2vec_model['中国']"]},{"cell_type":"markdown","metadata":{},"source":["### 8. 相似度测试"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["金融\n","[('金融服务', 0.7745651006698608), ('证券期货', 0.772100031375885), ('信贷', 0.7595240473747253), ('投资银行', 0.7579774260520935), ('国际金融', 0.7491804361343384), ('银行学', 0.7486739158630371), ('金融保险', 0.7417786717414856), ('保险业', 0.7401484251022339), ('期货', 0.7384737133979797), ('金融市场', 0.738309383392334)]\n","喜欢\n","[('讨厌', 0.7161690592765808), ('吃喝', 0.6951901912689209), ('很会', 0.6930608153343201), ('喝酒', 0.6888067722320557), ('吓人', 0.6877806186676025), ('偏爱', 0.686855137348175), ('迟钝', 0.6796375513076782), ('卖弄', 0.6752473711967468), ('没什么', 0.6695799231529236), ('鞋子', 0.6682382822036743)]\n","中国\n","[('大陆', 0.712977409362793), ('顾诚', 0.6032476425170898), ('李泽厚', 0.6030479669570923), ('榜上有名', 0.6003459692001343), ('世界史', 0.5973314046859741), ('中国地图出版社', 0.5888921618461609), ('内地', 0.5883994102478027), ('少帅', 0.5853191018104553), ('戏曲史', 0.5793296098709106), ('瑰宝', 0.5780259370803833)]\n","北京\n","[('上海', 0.7301627993583679), ('天津', 0.7032418847084045), ('杭州', 0.6845793128013611), ('北京市', 0.660686194896698), ('沈阳', 0.6576042175292969), ('南京', 0.6466558575630188), ('上海市', 0.6343791484832764), ('武汉', 0.6275659203529358), ('西安', 0.618435263633728), ('首都国际机场', 0.6135655641555786)]\n"]}],"source":["testwords = ['金融', '喜欢', \"中国\", \"北京\"]\n","for word in testwords:\n","    res = word2vec_model.most_similar(word)\n","    print (word)\n","    print (res)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
