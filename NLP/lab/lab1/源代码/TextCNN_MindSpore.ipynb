{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 1. 数据同步"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:root:Using MoXing-v1.17.3-d858ff4a\n","INFO:root:Using OBS-Python-SDK-3.20.9.1\n"]}],"source":["import moxing as mox\n","# 请替换成自己的obs路径\n","mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\", dst_url='./data/') "]},{"cell_type":"markdown","metadata":{},"source":["## 2. 导入依赖库"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","import numpy as np\n","import pandas as pd\n","import os\n","import math\n","import random\n","import codecs\n","from pathlib import Path\n","\n","import mindspore\n","import mindspore.dataset as ds\n","import mindspore.nn as nn\n","from mindspore import Tensor\n","from mindspore import context\n","from mindspore.train.model import Model\n","from mindspore.nn.metrics import Accuracy\n","from mindspore.train.serialization import load_checkpoint, load_param_into_net\n","from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n","from mindspore.ops import operations as ops"]},{"cell_type":"markdown","metadata":{},"source":["## 3. 超参数设置"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["from easydict import EasyDict as edict\n","\n","cfg = edict({\n","    'name': 'movie review',\n","    'pre_trained': False,\n","    'num_classes': 2,\n","    'batch_size': 64,\n","    'epoch_size': 4,\n","    'weight_decay': 3e-5,\n","    'data_path': './data/',\n","    'device_target': 'Ascend',\n","    'device_id': 0,\n","    'keep_checkpoint_max': 1,\n","    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n","    'word_len': 51,\n","    'vec_length': 40\n","})"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"]},{"cell_type":"markdown","metadata":{},"source":["## 4. 数据预处理"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Negative reivews:\n","[0]:simplistic , silly and tedious . \n","\n","[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n","\n","[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n","\n","[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n","\n","[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n","\n","Positive reivews:\n","[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n","\n","[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n","\n","[2]:effective but too-tepid biopic\n","\n","[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n","\n","[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n","\n"]}],"source":["# 数据预览\n","with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n","        print(\"Negative reivews:\")\n","        for i in range(5):\n","            print(\"[{0}]:{1}\".format(i,f.readline()))\n","with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n","        print(\"Positive reivews:\")\n","        for i in range(5):\n","            print(\"[{0}]:{1}\".format(i,f.readline()))"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["class Generator():\n","    def __init__(self, input_list):\n","        self.input_list=input_list\n","    def __getitem__(self,item):\n","        return (np.array(self.input_list[item][0],dtype=np.int32),\n","                np.array(self.input_list[item][1],dtype=np.int32))\n","    def __len__(self):\n","        return len(self.input_list)\n","\n","\n","class MovieReview:\n","    '''\n","    影评数据集\n","    '''\n","    def __init__(self, root_dir, maxlen, split):\n","        '''\n","        input:\n","            root_dir: 影评数据目录\n","            maxlen: 设置句子最大长度\n","            split: 设置数据集中训练/评估的比例\n","        '''\n","        self.path = root_dir\n","        self.feelMap = {\n","            'neg':0,\n","            'pos':1\n","        }\n","        self.files = []\n","\n","        self.doConvert = False\n","        \n","        mypath = Path(self.path)\n","        if not mypath.exists() or not mypath.is_dir():\n","            print(\"please check the root_dir!\")\n","            raise ValueError\n","\n","        # 在数据目录中找到文件\n","        for root,_,filename in os.walk(self.path):\n","            for each in filename:\n","                self.files.append(os.path.join(root,each))\n","            break\n","\n","        # 确认是否为两个文件.neg与.pos\n","        if len(self.files) != 2:\n","            print(\"There are {} files in the root_dir\".format(len(self.files)))\n","            raise ValueError\n","\n","        # 读取数据\n","        self.word_num = 0\n","        self.maxlen = 0\n","        self.minlen = float(\"inf\")\n","        self.maxlen = float(\"-inf\")\n","        self.Pos = []\n","        self.Neg = []\n","        for filename in self.files:\n","            f = codecs.open(filename, 'r')\n","            ff = f.read()\n","            file_object = codecs.open(filename, 'w', 'utf-8')\n","            file_object.write(ff)\n","            self.read_data(filename)\n","        self.PosNeg = self.Pos + self.Neg\n","\n","        self.text2vec(maxlen=maxlen)\n","        self.split_dataset(split=split)\n","\n","    def read_data(self, filePath):\n","\n","        with open(filePath,'r') as f:\n","            \n","            for sentence in f.readlines():\n","                sentence = sentence.replace('\\n','')\\\n","                                    .replace('\"','')\\\n","                                    .replace('\\'','')\\\n","                                    .replace('.','')\\\n","                                    .replace(',','')\\\n","                                    .replace('[','')\\\n","                                    .replace(']','')\\\n","                                    .replace('(','')\\\n","                                    .replace(')','')\\\n","                                    .replace(':','')\\\n","                                    .replace('--','')\\\n","                                    .replace('-',' ')\\\n","                                    .replace('\\\\','')\\\n","                                    .replace('0','')\\\n","                                    .replace('1','')\\\n","                                    .replace('2','')\\\n","                                    .replace('3','')\\\n","                                    .replace('4','')\\\n","                                    .replace('5','')\\\n","                                    .replace('6','')\\\n","                                    .replace('7','')\\\n","                                    .replace('8','')\\\n","                                    .replace('9','')\\\n","                                    .replace('`','')\\\n","                                    .replace('=','')\\\n","                                    .replace('$','')\\\n","                                    .replace('/','')\\\n","                                    .replace('*','')\\\n","                                    .replace(';','')\\\n","                                    .replace('<b>','')\\\n","                                    .replace('%','')\n","                sentence = sentence.split(' ')\n","                sentence = list(filter(lambda x: x, sentence))\n","                if sentence:\n","                    self.word_num += len(sentence)\n","                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n","                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n","                    if 'pos' in filePath:\n","                        self.Pos.append([sentence,self.feelMap['pos']])\n","                    else:\n","                        self.Neg.append([sentence,self.feelMap['neg']])\n","\n","    def text2vec(self, maxlen):\n","        '''\n","        将句子转化为向量\n","\n","        '''\n","        # Vocab = {word : index}\n","        self.Vocab = dict()\n","\n","        # self.Vocab['None']\n","        for SentenceLabel in self.Pos+self.Neg:\n","            vector = [0]*maxlen\n","            for index, word in enumerate(SentenceLabel[0]):\n","                if index >= maxlen:\n","                    break\n","                if word not in self.Vocab.keys():\n","                    self.Vocab[word] = len(self.Vocab)\n","                    vector[index] = len(self.Vocab) - 1\n","                else:\n","                    vector[index] = self.Vocab[word]\n","            SentenceLabel[0] = vector\n","        self.doConvert = True\n","\n","    def split_dataset(self, split):\n","        '''\n","        分割为训练集与测试集\n","\n","        '''\n","\n","        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n","        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n","        trunk_num = int(1/(1-split))\n","        pos_temp=list()\n","        neg_temp=list()\n","        for index in range(trunk_num):\n","            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n","            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n","        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n","        self.train = [i for item in pos_temp+neg_temp for i in item]\n","\n","        random.shuffle(self.train)\n","        # random.shuffle(self.test)\n","\n","    def get_dict_len(self):\n","        '''\n","        获得数据集中文字组成的词典长度\n","        '''\n","        if self.doConvert:\n","            return len(self.Vocab)\n","        else:\n","            print(\"Haven't finished Text2Vec\")\n","            return -1\n","\n","    def create_train_dataset(self, epoch_size, batch_size):\n","        dataset = ds.GeneratorDataset(\n","                                        source=Generator(input_list=self.train), \n","                                        column_names=[\"data\",\"label\"], \n","                                        shuffle=False\n","                                        )\n","#         dataset.set_dataset_size(len(self.train))\n","        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n","        dataset=dataset.repeat(epoch_size)\n","        return dataset\n","\n","    def create_test_dataset(self, batch_size):\n","        dataset = ds.GeneratorDataset(\n","                                        source=Generator(input_list=self.test), \n","                                        column_names=[\"data\",\"label\"], \n","                                        shuffle=False\n","                                        )\n","#         dataset.set_dataset_size(len(self.test))\n","        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n","        return dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n","dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n","batch_num = dataset.get_dataset_size() "]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab_size:18848\n","{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n","[[   15,  3190,  6781 ...     0,     0,     0],\n"," [ 1320,   582,     4 ...     0,     0,     0],\n"," [ 1734,   111,    36 ...     0,     0,     0],\n"," ...\n"," [   82,    94,   367 ...     0,     0,     0],\n"," [10449,    55,  2923 ...     0,     0,     0],\n"," [  336,   203,   272 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, \n"," 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, \n"," 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1])}\n","[ 1320   582     4  3070     0   603  5507 12780    32 12781  1304   669\n","   896  1310   122     4    82     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0]\n"]}],"source":["vocab_size=instance.get_dict_len()\n","print(\"vocab_size:{0}\".format(vocab_size))\n","item =dataset.create_dict_iterator()\n","for i,data in enumerate(item):\n","    if i<1:\n","        print(data)\n","        print(data['data'][1])\n","    else:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["## 5.模型训练"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1训练参数设置"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["learning_rate = []\n","warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n","           for i in range(math.floor(cfg.epoch_size / 5))]\n","shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n","          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n","normal_run = [1e-3 for _ in range(batch_num) for i in \n","              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n","                    - math.floor(cfg.epoch_size * 2 / 5))]\n","learning_rate = learning_rate + warm_up + normal_run + shrink"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def _weight_variable(shape, factor=0.01):\n","    init_value = np.random.randn(*shape).astype(np.float32) * factor\n","    return Tensor(init_value)\n","\n","\n","def make_conv_layer(kernel_size):\n","    weight_shape = (96, 1, *kernel_size)\n","    weight = _weight_variable(weight_shape)\n","    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n","                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n","\n","\n","class TextCNN(nn.Cell):\n","    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n","        super(TextCNN, self).__init__()\n","        self.vec_length = vec_length\n","        self.word_len = word_len\n","        self.num_classes = num_classes\n","\n","        self.unsqueeze = ops.ExpandDims()\n","        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n","\n","        self.slice = ops.Slice()\n","        self.layer1 = self.make_layer(kernel_height=3)\n","        self.layer2 = self.make_layer(kernel_height=4)\n","        self.layer3 = self.make_layer(kernel_height=5)\n","\n","        self.concat = ops.Concat(1)\n","\n","        self.fc = nn.Dense(96*3, self.num_classes)\n","        self.drop = nn.Dropout(keep_prob=0.5)\n","        self.print = ops.Print()\n","        self.reducemean = ops.ReduceMax(keep_dims=False)\n","        \n","    def make_layer(self, kernel_height):\n","        return nn.SequentialCell(\n","            [\n","                make_conv_layer((kernel_height,self.vec_length)),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n","            ]\n","        )\n","\n","    def construct(self,x):\n","        x = self.unsqueeze(x, 1)\n","        x = self.embedding(x)\n","        x1 = self.layer1(x)\n","        x2 = self.layer2(x)\n","        x3 = self.layer3(x)\n","\n","        x1 = self.reducemean(x1, (2, 3))\n","        x2 = self.reducemean(x2, (2, 3))\n","        x3 = self.reducemean(x3, (2, 3))\n","\n","        x = self.concat((x1, x2, x3))\n","        x = self.drop(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n","              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TextCNN<\n","  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table), dtype=Float32, padding_idx=None>\n","  (layer1): SequentialCell<\n","    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[-1.01975221e-02 -6.93383277e-04  4.07581311e-03 ... -4.57499130e-03\n","         2.05630157e-03 -3.86761921e-03]\n","       [ 5.27941715e-03 -5.78200154e-04  1.88886896e-02 ... -1.42225558e-02\n","         3.56013328e-03 -1.76590439e-02]\n","       [-1.47636037e-03  1.24138523e-05  2.62513896e-03 ...  4.56628995e-03\n","        -4.60443925e-03  5.23316732e-04]]]\n","    \n","    \n","     [[[ 1.80724217e-03  5.58177708e-03  9.58025362e-03 ...  8.16981215e-03\n","         2.29106238e-03  1.75089308e-03]\n","       [ 2.15005642e-03 -1.70884822e-02 -2.57208943e-03 ... -5.47323236e-03\n","         2.04329342e-02 -2.61869072e-03]\n","       [ 8.53526313e-03 -1.11518183e-03  6.77704671e-03 ... -1.09469565e-02\n","        -8.85556859e-04  1.62346419e-02]]]\n","    \n","    \n","     [[[ 1.05571337e-02  1.02827270e-02  7.08943466e-03 ... -7.58388219e-03\n","        -6.97487779e-03 -5.68528380e-03]\n","       [-1.69466611e-03 -8.01722333e-03 -2.59528006e-03 ...  7.05966214e-03\n","         8.20585247e-03  1.51339360e-02]\n","       [-1.10727474e-02 -4.44684690e-03  4.66134585e-03 ...  2.85023329e-04\n","         1.45670259e-02 -1.30835073e-02]]]\n","    \n","    \n","     ...\n","    \n","    \n","     [[[ 7.29244552e-04 -8.12886516e-04  7.41507672e-03 ... -4.10588272e-03\n","         5.37596527e-04 -2.40132911e-03]\n","       [-1.19297989e-02  6.60708547e-03  6.75804971e-04 ... -3.80326062e-03\n","        -1.17351841e-02 -2.40278011e-03]\n","       [ 4.79531894e-03 -5.88043639e-03 -1.48151498e-02 ...  2.10125046e-03\n","        -6.45607943e-04 -5.41895395e-03]]]\n","    \n","    \n","     [[[-1.83240825e-03  5.43291261e-03  7.10784039e-03 ... -2.70072818e-02\n","         8.53812380e-04  2.29081349e-03]\n","       [ 1.84109248e-02  1.83647301e-03  6.15894038e-04 ...  6.07677922e-03\n","         1.48664592e-02  3.42242979e-03]\n","       [ 2.40820441e-02 -4.23888676e-03  1.56482570e-02 ... -1.89700369e-02\n","         4.46266774e-03  2.08784896e-03]]]\n","    \n","    \n","     [[[-5.29482728e-03 -9.43536963e-03 -6.72342116e-03 ... -3.29096220e-03\n","        -1.00514204e-04 -1.27961505e-02]\n","       [-7.11471424e-04  3.76987882e-04  9.77989007e-03 ... -9.41509008e-03\n","         5.21176634e-03 -1.84813328e-02]\n","       [-1.04572345e-02 -2.32955371e-03 -6.10041525e-03 ...  9.02488921e-03\n","         1.38039589e-02  6.35558600e-03]]]], bias_init=zeros, format=NCHW>\n","    (1): ReLU<>\n","    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n","    >\n","  (layer2): SequentialCell<\n","    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[ 3.4271076e-03  8.9845415e-03  1.7176390e-02 ... -2.9000149e-03\n","         1.0413056e-02  1.8878920e-02]\n","       [ 1.2836415e-02  2.7134572e-04 -3.2851091e-03 ...  1.3853388e-03\n","         1.5680760e-02 -1.5339485e-02]\n","       [ 5.2072885e-03 -2.7773778e-03 -7.9283314e-03 ... -7.0501119e-03\n","        -6.2660412e-03  5.3412626e-03]\n","       [ 9.1624185e-03  6.1636638e-05  1.1323442e-02 ... -1.8328846e-03\n","         1.0890803e-02  8.1075635e-03]]]\n","    \n","    \n","     [[[ 2.1277487e-02  7.7600144e-03 -1.6961701e-02 ...  1.2806475e-03\n","         3.5466203e-03  4.2962562e-03]\n","       [-7.0943398e-04  9.7264542e-04  5.6935935e-03 ...  2.1027619e-02\n","         1.1466548e-02  5.8571142e-03]\n","       [-5.5385237e-03  1.4731588e-03 -1.8708872e-02 ...  6.8696742e-03\n","         1.1364183e-02  2.8502978e-02]\n","       [-2.4210294e-03  6.9721495e-03  6.7677801e-03 ... -1.5072146e-02\n","         3.7238661e-03 -1.7722677e-02]]]\n","    \n","    \n","     [[[ 2.0841356e-02 -5.5947839e-03  1.5548697e-03 ... -2.8236578e-03\n","         2.8949368e-03 -4.3771490e-03]\n","       [ 2.2186600e-03 -1.1632005e-02  9.4650529e-04 ... -2.7542054e-03\n","         3.2039597e-03 -2.5674473e-03]\n","       [ 7.5164917e-03 -7.9974737e-03  6.6294358e-03 ... -2.0571537e-02\n","        -6.1667776e-03 -7.7378131e-03]\n","       [ 1.3070641e-02  1.2145782e-03  6.9544637e-03 ... -8.3221644e-03\n","         7.6622861e-03  2.3599975e-04]]]\n","    \n","    \n","     ...\n","    \n","    \n","     [[[ 1.2157369e-02  2.2094753e-02  8.2681170e-03 ...  7.0307413e-03\n","         1.7739513e-03 -5.7638143e-03]\n","       [-1.0695128e-02 -1.1607397e-03  7.7116708e-03 ...  7.9374947e-03\n","        -4.1872882e-03 -4.9673435e-03]\n","       [-2.8225856e-03  6.0150847e-03  2.9882353e-03 ... -3.7366797e-03\n","         6.7756353e-03 -1.4647303e-02]\n","       [-1.8517365e-03  2.9673185e-03 -7.3632919e-03 ...  7.0564396e-04\n","        -1.3410307e-02 -4.0863943e-03]]]\n","    \n","    \n","     [[[-1.2848447e-03 -3.0739822e-03  1.6846674e-02 ... -9.1170659e-03\n","        -9.0347296e-03 -1.9096570e-02]\n","       [-1.0032056e-02  2.3219537e-03 -4.1512656e-03 ... -8.9047372e-04\n","         2.8588218e-04  1.7372811e-02]\n","       [ 8.7444866e-03  3.8105189e-03  5.0835032e-03 ...  3.9129863e-03\n","         1.0350514e-02  1.7767347e-02]\n","       [-1.3506732e-02 -1.4255235e-02 -1.5193651e-02 ...  3.9721983e-03\n","         9.0690823e-03 -2.4886047e-02]]]\n","    \n","    \n","     [[[ 9.2416350e-03  1.4977989e-02  2.1778185e-02 ...  3.9517372e-03\n","        -8.2300534e-04  5.9668249e-03]\n","       [-5.4674363e-03  2.9111996e-03 -1.6778689e-02 ...  1.0264850e-03\n","         7.9059467e-04 -8.4615238e-03]\n","       [-4.1162083e-03 -2.4206988e-03  8.8477591e-03 ...  1.7782494e-03\n","         4.5224330e-03 -4.7719896e-05]\n","       [-1.4600134e-02 -6.6143721e-03 -9.8061254e-03 ...  9.7008441e-03\n","        -6.5608607e-03  1.3068032e-04]]]], bias_init=zeros, format=NCHW>\n","    (1): ReLU<>\n","    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n","    >\n","  (layer3): SequentialCell<\n","    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[-2.21139286e-03 -2.41488521e-03  2.13555936e-02 ... -1.65794473e-02\n","        -7.10360997e-04 -1.62575231e-03]\n","       [ 1.11704292e-02 -1.78829674e-02 -1.14661106e-03 ... -1.14609059e-02\n","        -3.68877198e-03 -4.50790540e-04]\n","       [-7.39941327e-03 -4.23143414e-04  1.97958127e-02 ... -9.11398139e-03\n","         8.92375072e-04  1.19612953e-02]\n","       [-4.74007474e-03  1.00720944e-02  3.63600045e-03 ...  1.67858973e-02\n","        -2.58720089e-02  8.07726383e-03]\n","       [-7.80701917e-03 -7.49023072e-03  2.31460156e-03 ... -5.95709542e-03\n","         1.25484234e-02 -7.18322815e-03]]]\n","    \n","    \n","     [[[ 4.05057101e-03  6.22314075e-03 -1.79613626e-03 ... -1.04579255e-02\n","         6.66824076e-03 -5.07642375e-03]\n","       [-6.33907551e-03 -5.24501549e-03  9.81074572e-03 ...  1.08612441e-02\n","         1.70226244e-03  7.94867240e-03]\n","       [ 4.94264718e-03 -1.35119352e-02 -7.92734139e-03 ... -1.54358121e-02\n","         7.29021558e-04  2.12917291e-02]\n","       [-1.20290322e-02  1.84835494e-03 -1.47105183e-03 ...  1.58773959e-02\n","         5.38740726e-03  1.24286087e-02]\n","       [-7.55051966e-04 -3.56710283e-03  7.49293063e-03 ...  1.35142896e-02\n","        -8.74322373e-03  3.62198614e-03]]]\n","    \n","    \n","     [[[ 1.23833949e-02 -6.38807379e-03 -2.67221536e-02 ... -9.67524713e-04\n","        -1.45942960e-02 -9.44472733e-04]\n","       [-9.08638909e-03 -9.78543237e-03 -3.33389640e-03 ... -3.80483689e-03\n","         1.16851386e-02  1.43156922e-03]\n","       [ 1.43256458e-02 -5.70932869e-03 -1.13590928e-02 ... -1.99383404e-02\n","        -6.43174350e-03  1.42686237e-02]\n","       [-1.46491197e-03 -5.53586870e-05 -3.01511330e-03 ... -1.41065866e-02\n","        -2.57367850e-03 -6.79661753e-03]\n","       [-4.87489766e-03  7.77473859e-03 -1.12401610e-02 ...  4.49080952e-03\n","        -4.50980524e-03 -8.48950935e-04]]]\n","    \n","    \n","     ...\n","    \n","    \n","     [[[ 7.76800327e-03 -9.63745266e-03  7.49069266e-03 ... -1.61703618e-03\n","        -1.16275484e-03  2.07886496e-03]\n","       [ 1.02279056e-03  1.56720411e-02  9.58159752e-03 ... -7.44965533e-03\n","         4.54464648e-03 -7.19113462e-03]\n","       [ 1.53354798e-02  4.20627730e-05 -1.24453008e-03 ... -3.67996749e-03\n","         3.53310304e-03 -1.08989011e-02]\n","       [ 3.17901955e-03 -4.66305204e-03  6.87431311e-03 ... -1.95163600e-02\n","         3.92241823e-03  1.11916359e-03]\n","       [ 2.58645299e-03 -1.48356836e-02  1.63387097e-02 ... -8.05650780e-04\n","        -1.30316457e-02 -3.01466876e-04]]]\n","    \n","    \n","     [[[ 3.19446251e-03  2.96817850e-02 -1.40278563e-02 ... -1.84968859e-02\n","         2.09035864e-03  2.98187509e-03]\n","       [-1.55713316e-02 -7.12589920e-03 -8.18836503e-03 ... -8.60458426e-03\n","        -9.79105849e-03 -1.56782549e-02]\n","       [-7.78470794e-03  9.11665242e-03  1.06965341e-02 ...  1.00400383e-02\n","         2.14654971e-02 -4.33663465e-03]\n","       [-3.12025397e-04 -1.52020631e-02 -1.80257801e-02 ... -5.49256569e-03\n","        -9.82420333e-03  7.44856708e-03]\n","       [ 2.09764205e-02  3.55819124e-03 -1.49949372e-03 ...  8.69765878e-03\n","        -2.15322757e-03 -9.84702073e-03]]]\n","    \n","    \n","     [[[ 1.28701515e-02  1.09110577e-02  1.85842607e-02 ...  7.26271654e-03\n","        -8.93867668e-03  5.60572185e-03]\n","       [-1.93308294e-02 -1.46030646e-03 -6.15600077e-03 ...  1.00303730e-02\n","        -7.92537536e-03  1.08167678e-02]\n","       [ 1.47010405e-02 -1.63789820e-02 -8.38982966e-03 ...  7.99456704e-03\n","        -4.81590442e-03  3.47689202e-04]\n","       [-5.46710053e-03 -1.18273115e-02 -4.41340409e-04 ... -2.90321792e-03\n","         4.37540584e-04 -3.17701045e-03]\n","       [-2.04848833e-02  4.65657981e-03  4.18039411e-03 ... -4.22161072e-04\n","        -1.47479621e-03 -2.24956870e-03]]]], bias_init=zeros, format=NCHW>\n","    (1): ReLU<>\n","    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n","    >\n","  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n","  (drop): Dropout<keep_prob=0.5>\n","  >\n"]}],"source":["print(net)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["# Continue training if set pre_trained to be True\n","if cfg.pre_trained:\n","    param_dict = load_checkpoint(cfg.checkpoint_path)\n","    load_param_into_net(net, param_dict)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n","              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n","loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n","time_cb = TimeMonitor(data_size=batch_num)\n","ckpt_save_dir = \"./ckpt\"\n","ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n","loss_cb = LossMonitor()"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 1 step: 596, loss is 0.07209684\n","epoch time: 39359.259 ms, per step time: 66.039 ms\n","epoch: 2 step: 596, loss is 0.0029934864\n","epoch time: 4308.688 ms, per step time: 7.229 ms\n","epoch: 3 step: 596, loss is 0.0019718197\n","epoch time: 4266.735 ms, per step time: 7.159 ms\n","epoch: 4 step: 596, loss is 0.0011571363\n","epoch time: 4309.405 ms, per step time: 7.231 ms\n","train success\n"]}],"source":["model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n","print(\"train success\")"]},{"cell_type":"markdown","metadata":{},"source":["## 6. 测试评估"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n","accuracy:  {'acc': 0.763671875}\n"]}],"source":["dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n","opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n","              learning_rate=0.001, weight_decay=cfg.weight_decay)\n","loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n","net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n","                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n","\n","if checkpoint_path is not None:\n","    param_dict = load_checkpoint(checkpoint_path)\n","    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n","else:\n","    param_dict = load_checkpoint(cfg.checkpoint_path)\n","    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n","\n","load_param_into_net(net, param_dict)\n","net.set_train(False)\n","model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n","\n","acc = model.eval(dataset)\n","print(\"accuracy: \", acc)"]},{"cell_type":"markdown","metadata":{},"source":["## 7. 在线测试"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess(sentence):\n","    sentence = sentence.lower().strip()\n","    sentence = sentence.replace('\\n','')\\\n","                                    .replace('\"','')\\\n","                                    .replace('\\'','')\\\n","                                    .replace('.','')\\\n","                                    .replace(',','')\\\n","                                    .replace('[','')\\\n","                                    .replace(']','')\\\n","                                    .replace('(','')\\\n","                                    .replace(')','')\\\n","                                    .replace(':','')\\\n","                                    .replace('--','')\\\n","                                    .replace('-',' ')\\\n","                                    .replace('\\\\','')\\\n","                                    .replace('0','')\\\n","                                    .replace('1','')\\\n","                                    .replace('2','')\\\n","                                    .replace('3','')\\\n","                                    .replace('4','')\\\n","                                    .replace('5','')\\\n","                                    .replace('6','')\\\n","                                    .replace('7','')\\\n","                                    .replace('8','')\\\n","                                    .replace('9','')\\\n","                                    .replace('`','')\\\n","                                    .replace('=','')\\\n","                                    .replace('$','')\\\n","                                    .replace('/','')\\\n","                                    .replace('*','')\\\n","                                    .replace(';','')\\\n","                                    .replace('<b>','')\\\n","                                    .replace('%','')\\\n","                                    .replace(\"  \",\" \")\n","    sentence = sentence.split(' ')\n","    maxlen = cfg.word_len\n","    vector = [0]*maxlen\n","    for index, word in enumerate(sentence):\n","        if index >= maxlen:\n","            break\n","        if word not in instance.Vocab.keys():\n","            print(word,\"单词未出现在字典中\")\n","        else:\n","            vector[index] = instance.Vocab[word]\n","    sentence = vector\n","\n","    return sentence\n","\n","def inference(review_en):\n","    review_en = preprocess(review_en)\n","    input_en = Tensor(np.array([review_en]).astype(np.int32))\n","    output = net(input_en)\n","    if np.argmax(np.array(output[0])) == 1:\n","        print(\"Positive comments\")\n","    else:\n","        print(\"Negative comments\")"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Negative comments\n"]}],"source":["review_en = \"the movie is so boring\"\n","inference(review_en)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"toc-showcode":true},"nbformat":4,"nbformat_minor":4}
