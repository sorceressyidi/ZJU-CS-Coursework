{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d0a452",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2599b",
   "metadata": {},
   "source": [
    "### 1. 安装gensim库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b39cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting gensim\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/9f/10/0a737b7f935a14ac49d12187530952805978e1be506212b7c66d15962e27/gensim-4.2.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.21.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.5.4)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ad/08/dcd19850b79f72e3717c98b2088f8a24b549b29ce66849cd6b7f44679683/smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-7.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479be3b3",
   "metadata": {},
   "source": [
    "### 2. 同步数据至本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e95ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/word_embedding/corpus.txt\", dst_url='corpus.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8458d12",
   "metadata": {},
   "source": [
    "### 3. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490c81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865f3b8",
   "metadata": {},
   "source": [
    "### 4. 定义输入、输出文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02206a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"corpus.txt\"\n",
    "out_embedding_file = \"embedding.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec554e65",
   "metadata": {},
   "source": [
    "### 5. 查看函数文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961dc6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
       "\n",
       "Warnings\n",
       "--------\n",
       "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
       "such as lambda functions etc.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
       "to reduce memory.\n",
       "\n",
       "The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "vector_size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "epochs : int, optional\n",
       "    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "shrink_windows : bool, optional\n",
       "    New in 4.1. Experimental.\n",
       "    If True, the effective window size is uniformly sampled from  [1, `window`]\n",
       "    for each target word during training, to match the original word2vec algorithm's\n",
       "    approximate weighting of context words by distance. Otherwise, the effective\n",
       "    window size is always fixed to `window` words to either side.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Doc2Vec, FastText\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde7777",
   "metadata": {},
   "source": [
    "### 6. 词向量训练并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5d3506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file=corpus_file, vector_size=100, window=5, min_count=5, workers=cpu_count(), sg=1)\n",
    "model.wv.save_word2vec_format(out_embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b15f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt  embedding.txt  word2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e642a4e",
   "metadata": {},
   "source": [
    "### 7. 加载离线词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd38096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fc777",
   "metadata": {},
   "source": [
    "获取单个词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bcb52cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2017746 ,  0.13339779,  0.06821822, -0.4609831 ,  0.02775645,\n",
       "        0.05074601,  0.3358123 ,  0.25146517,  0.65093803, -0.05355043,\n",
       "       -0.2679391 , -0.77372897, -0.07044063, -0.22773655, -0.02749912,\n",
       "        0.1389289 ,  0.3468083 , -0.17417955,  0.3082262 , -0.36638096,\n",
       "        0.46656582,  0.33796635,  0.21645749,  0.15020363, -0.64400166,\n",
       "        0.11008333,  0.45217153,  0.20718049,  0.00185695, -0.6839598 ,\n",
       "        0.40335184, -0.30479342,  0.31836027, -0.34140754,  0.0084216 ,\n",
       "       -0.21584107,  1.0167415 ,  0.5353181 ,  0.09963246,  0.41134953,\n",
       "       -0.0702543 , -0.34524864, -0.03512657, -0.49889746,  0.74877936,\n",
       "        0.26475936,  0.02005244,  0.522911  ,  0.09194136, -0.1541824 ,\n",
       "       -0.0362724 ,  0.42351776,  0.250849  , -0.4307574 ,  0.02435812,\n",
       "       -0.02017842,  0.35787287,  0.03121399, -0.65222853,  0.45418575,\n",
       "        0.35848376, -0.5165515 , -0.00857024,  0.37892684,  0.5917599 ,\n",
       "        0.4763419 , -0.33813712,  0.22763523, -0.12484685, -0.16329892,\n",
       "        0.33379993, -0.02850113, -0.62778825, -0.07337441, -0.1581671 ,\n",
       "       -0.09319846,  0.47340587,  0.44441393,  0.22871685,  0.24318066,\n",
       "        0.35177135,  0.09340252,  0.3799419 , -0.10534132,  0.49854138,\n",
       "       -0.3581865 ,  0.35577983, -0.23150563, -0.26745605, -0.06827668,\n",
       "       -0.13000032, -0.9860383 ,  0.32067043,  0.32912895,  0.16230156,\n",
       "       -0.07491212,  0.1546822 , -0.03170412, -0.5346847 , -0.05118856],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['中国']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ac5e7",
   "metadata": {},
   "source": [
    "### 8. 相似度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35555c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融\n",
      "[('房地产', 0.7646191716194153), ('金融服务', 0.7605745196342468), ('银行学', 0.7537165880203247), ('证券期货', 0.7518970966339111), ('信贷', 0.7516815066337585), ('国际金融', 0.7418786287307739), ('投资银行', 0.7400544285774231), ('金融保险', 0.7375705242156982), ('国际贸易', 0.7313641905784607), ('金融市场', 0.7304650545120239)]\n",
      "喜欢\n",
      "[('讨厌', 0.7337337732315063), ('吓人', 0.6922650337219238), ('很会', 0.6918137669563293), ('惬意', 0.6893380284309387), ('没够', 0.6879456043243408), ('喝酒', 0.6879323720932007), ('鞋子', 0.6858315467834473), ('迟钝', 0.6833765506744385), ('好看', 0.6807959675788879), ('单恋', 0.6801393628120422)]\n",
      "中国\n",
      "[('大陆', 0.7445324063301086), ('内地', 0.5959519743919373), ('少帅', 0.5898768901824951), ('顾诚', 0.585955023765564), ('中国地图出版社', 0.5768052339553833), ('黑龙江人民出版社', 0.5724115967750549), ('热血传奇', 0.5708552598953247), ('宁波人', 0.5699015259742737), ('百富榜', 0.5674355030059814), ('胡润', 0.5668639540672302)]\n",
      "北京\n",
      "[('天津', 0.7291107177734375), ('上海', 0.713751494884491), ('北京市', 0.6659596562385559), ('南京', 0.6618322134017944), ('京郊', 0.6372664570808411), ('西安', 0.6177608370780945), ('北京城', 0.6169923543930054), ('门今', 0.6135876178741455), ('杭州', 0.6135865449905396), ('自沉于', 0.6063312292098999)]\n"
     ]
    }
   ],
   "source": [
    "testwords = ['金融', '喜欢', \"中国\", \"北京\"]\n",
    "for word in testwords:\n",
    "    res = word2vec_model.most_similar(word)\n",
    "    print (word)\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db71dd",
   "metadata": {},
   "source": [
    "### 9. 词向量文件回传obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"embedding.txt\", dst_url='s3://ascend-zyjs-dcyang/nlp/word_embedding/embedding.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
